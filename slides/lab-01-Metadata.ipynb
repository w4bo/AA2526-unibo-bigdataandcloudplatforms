{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/w4bo/AA2425-unibo-bigdataandcloudplatforms/blob/main/slides/lab-01-Metadata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLtjKHj97vTU"
   },
   "source": [
    "# The `California Housing Pricing` case study\n",
    "\n",
    "This notebook runs on Google Colab.\n",
    "\n",
    "- Colab provides a serverless Jupyter notebook environment for interactive development.\n",
    "- (At the moment, 2024) Google Colab is free to use like other G Suite products.\n",
    "\n",
    "In this laboratory we will build a simple data pipeline to get acquainted with the \"main\" steps necessary to transform your data.\n",
    "\n",
    "- The data contains information from the 1990 California census. It does provide an accessible introductory dataset for teaching people about the basics of machine learning. \n",
    "\n",
    "From the book [Hands on machine learning](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
    "\n",
    "> This data has metrics such as the population, median income, median housing price, and so on for each block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). We will just call them “districts” for short. The goal is to build a model to predict the median housing price in any district, given all the other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0QiiiaUHUpW"
   },
   "source": [
    "# Setup (& library versioning)\n",
    "\n",
    "First of all, we need to setup the Python environment by installing and importing the necessary Python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-MhsLhYHUpY",
    "outputId": "cfb85ad7-1e21-4806-e457-682013a988b5"
   },
   "outputs": [],
   "source": [
    "!pip install prov pydot\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import prov\n",
    "\n",
    "print(pd.__version__)\n",
    "print(sk.__version__)\n",
    "print(np.__version__)\n",
    "print(sns.__version__)\n",
    "print(prov.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why should we track the libraries imported in the coding environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMzMkSBOC8a5"
   },
   "source": [
    "# Data collection\n",
    "\n",
    "Import the dataset. In this case, there is no need for ETL/integration since the dataset is ready for elaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "NIoIXwyo8M7h",
    "outputId": "f35e6b70-3545-446e-ce67-d62f2f81fb4b"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://w4bo.github.io/AA2324-unibo-bigdataandcloudplatforms/housing.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzkPHfKdHUpe"
   },
   "source": [
    "# Profiling: Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqSwzJb1DH3F"
   },
   "source": [
    "Schema description\n",
    "\n",
    "1. `longitude`: A measure of how far west a house is; a higher value is farther west\n",
    "2. `latitude`: A measure of how far north a house is; a higher value is farther north\n",
    "3. `housingMedianAge`: Median age of a house within a block; a lower number is a newer building\n",
    "4. `totalRooms`: Total number of rooms within a block\n",
    "5. `totalBedrooms`: Total number of bedrooms within a block\n",
    "6. `population`: Total number of people residing within a block\n",
    "7. `households`: Total number of households, a group of people residing within a home unit, for a block\n",
    "8. `medianIncome`: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "9. `medianHouseValue`: Median house value for households within a block (measured in US Dollars)\n",
    "10. `oceanProximity`: Location of the house w.r.t ocean/sea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgS5cgINHUph"
   },
   "source": [
    "# Profiling: Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgxApYX69xfM",
    "outputId": "ba0c0411-41df-44ff-9a89-4ecfcd40077c"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jekguUOOHUpk"
   },
   "source": [
    "# Profiling: Distribution and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "EXmTkaz6HUpn",
    "outputId": "c70f48cf-1080-487a-922e-488b3252f5dd"
   },
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNzioS6F-w4w"
   },
   "source": [
    "# Profiling: Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 776
    },
    "id": "yIl7LGkI-z7J",
    "outputId": "3850aafe-7d6f-4c20-a0a9-e404780b524c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.hist(bins=50, figsize=(16, 9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZHmiuOH-9qR"
   },
   "source": [
    "# Profiling: are there relationships between variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "L9fEBIAL-8uA",
    "outputId": "04297e5d-6e1d-4b41-9964-05fdb55cf746"
   },
   "outputs": [],
   "source": [
    "tmp = df[[\"median_income\", \"housing_median_age\", \"median_house_value\", \"households\", \"population\", \"total_rooms\"]]\n",
    "sns.pairplot(tmp.sample(n=1000, random_state=42), markers='o') # hue=\"median_house_value\",\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIEJz45c92j0"
   },
   "source": [
    "# Compression: Memory usage\n",
    "\n",
    "What if I change float64 to float32?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CB8oUKS91KX",
    "outputId": "2ab1a8d0-f625-43f6-9133-13cde54fcc4a"
   },
   "outputs": [],
   "source": [
    "dff = df.copy(deep=True) # copy the dataframe\n",
    "for x in df.columns: # iterate over the columns\n",
    "    if dff[x].dtype == 'float64': dff[x] = dff[x].astype('float32') # ... change it to `float32`\n",
    "dff.info() # show some statistics on the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression: Memory usage\n",
    "\n",
    "What if I change float64 to float16?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df.copy(deep=True) # copy the dataframe\n",
    "for x in df.columns: # iterate over the columns\n",
    "    if dff[x].dtype == 'float64': dff[x] = dff[x].astype('float16') # ... change it to `float16`\n",
    "dff.info() # show some statistics on the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJpz2_j6PBSP"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXoRiu4jDMIt"
   },
   "source": [
    "# Missing values\n",
    "\n",
    "There are some missing values in the column `total_bedorooms` what can we do?\n",
    "\n",
    "Most Machine Learning algorithms cannot work with missing features. We have three options:\n",
    "\n",
    "- Get rid of the corresponding districts (i.e., drop the rows)\n",
    "- Get rid of the whole attribute (i.e., drop the columns)\n",
    "- Set the values to some value (zero, the mean, the median, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkmSKRxF-GYN"
   },
   "source": [
    "# Non-numeric attributes\n",
    "\n",
    "`ocean_proximity` is a text attribute so we cannot compute its median. Some options:\n",
    "\n",
    "- Get rid of the whole attribute. (`df.drop(\"ocean_proximity\", axis=1)`)\n",
    "- Change from categorical to ordinal (e.g., `NEAR BAY` = 0, `INLAND` = 1)\n",
    "- Change from categorical to one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGctGA91_Fg3"
   },
   "source": [
    "# Scaling attributes\n",
    "\n",
    "Attributes have very different scales.\n",
    "\n",
    "Should we scale them?\n",
    "\n",
    "- Min-max normalization\n",
    "- Standardization\n",
    "- Robust scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ytYEaEv_Yr2"
   },
   "source": [
    "# Machine learning\n",
    "\n",
    "Our machine learning pipeline can be composed by alternative solutions\n",
    "\n",
    "If we consider the default parameters for each algorithm, we have\n",
    "\n",
    "- 3 options for imputation\n",
    "- ... x 2 options for encoding\n",
    "- ... x 3 options for normalization\n",
    "- ... x 3 algorithms\n",
    "- = 54 alternatives!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CYGTEjAHUpz"
   },
   "source": [
    "# Alternative pre-processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuflsEzF8jCv"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if \"ocean_proximity\" in df.columns:  # For now we simply drop \"ocean_proximity\"\n",
    "  df = df.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "# Let's create some dataset variations\n",
    "# dataset1: drop the rows containing the null values and the columns `latitude` and `longitude`\n",
    "dataset_v1 = df.copy(deep=True).dropna().drop([\"longitude\", \"latitude\"], axis=1)\n",
    "# dataset2: impute missing values with the average number of bedrooms\n",
    "dataset_v2 = df.copy(deep=True)\n",
    "dataset_v2[\"total_bedrooms\"] = dataset_v2[\"total_bedrooms\"].fillna(dataset_v2[\"total_bedrooms\"].mean())\n",
    "# dataset3: also standardize dataset_v2\n",
    "numerical_features = dataset_v2.select_dtypes(include=np.number)  # select numerical features\n",
    "scaler = StandardScaler()  # Create a StandardScaler object\n",
    "scaled_features = scaler.fit_transform(numerical_features)  # Fit and transform the numerical features\n",
    "dataset_v3 = pd.DataFrame(scaled_features, columns=numerical_features.columns)  # Convert the scaled features back to a DataFrame\n",
    "\n",
    "# Create the list of datasets\n",
    "datasets = [(dataset_v1, \"dataset_v1\"), (dataset_v2, \"dataset_v2\"), (dataset_v3, \"dataset_v3\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEs8uhoxHUp0"
   },
   "source": [
    "# Alternative machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqVj0Myk9Fju"
   },
   "outputs": [],
   "source": [
    "# Let's import some machine learning models (here we are not addressing hyper-parameter tuning)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "dt_3 = DecisionTreeRegressor(random_state=0, max_depth=3)  # initialize decision tree regressor model\n",
    "dt_5 = DecisionTreeRegressor(random_state=0, max_depth=5)  # initialize decision tree regressor model\n",
    "lr = LinearRegression()  # initialize a linear regressor model\n",
    "# Create the list of algorithms\n",
    "ml_algorithms = [(lr, \"lr\"), (dt_3, \"dt_3\"), (dt_5, \"dt_5\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7K4wux4HUp2"
   },
   "source": [
    "# Train the models\n",
    "\n",
    "What are the insights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "vqy4T65c9pBr",
    "outputId": "3940b1a7-1d26-40a6-e423-203e3c7cd50f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "instances, i = [], 0\n",
    "for dataset, dataset_version in datasets:  # For each dataset version...\n",
    "  X = dataset.drop(columns=[\"median_house_value\"]).to_numpy()  # Get the training set\n",
    "  y = dataset[\"median_house_value\"].to_numpy()  # Get the label array\n",
    "  for ml_algorithm, ml_algorithm_version in ml_algorithms:  # For each machine learning algorithm...\n",
    "    instance = {}  # Run the machine learning algorithm on the given dataset\n",
    "    instance[\"id\"] = i  # store the id of the instance\n",
    "    instance[\"dataset\"] = dataset_version  # store the version of the dataset\n",
    "    instance[\"algorithm\"] = ml_algorithm_version  # store the version of the ml algorithm\n",
    "    instance[\"score\"] = cross_val_score(ml_algorithm, X, y, cv=10).mean()  # store the performance of the pipeline instance\n",
    "    instances = instances + [instance]\n",
    "    i += 1\n",
    "result = pd.DataFrame.from_dict(instances, orient='columns')  # Collect the results\n",
    "sns.catplot(x = \"dataset\", y = \"score\", hue = \"algorithm\", data = result, kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIBtLp5PLnel"
   },
   "source": [
    "# How do we track all these changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdJz0jCOHUp7",
    "outputId": "03f8b34f-2ca5-472d-88ec-ddee58b51823"
   },
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| output: false\n",
    "\n",
    "!apt update -y\n",
    "!apt install graphviz -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and plotting the provenance graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "V5zvVzyBFEdm",
    "outputId": "bbd3f2da-7d24-47b0-b152-1bfce6166bd4"
   },
   "outputs": [],
   "source": [
    "from prov.model import ProvDocument\n",
    "from prov.dot import prov_to_dot\n",
    "from IPython.display import Image\n",
    "d1 = ProvDocument()  # Create an empty provenance document\n",
    "d1.add_namespace('unibo', 'https://www.unibo.it')  # add the namespace\n",
    "d1.add_namespace('sk', 'https://scikit-learn.org/stable/')  # add the namespace\n",
    "agent = d1.agent('unibo:mfrancia')  # add an agent\n",
    "d1.wasDerivedFrom(\"unibo:dataset_v3\", \"unibo:dataset_v2\")\n",
    "for dataset, dataset_version in datasets:  # For each dataset version...\n",
    "  original_dataset = d1.entity('unibo:' + dataset_version)  # register the dataset\n",
    "  d1.wasAttributedTo(original_dataset, agent)  # attribute the dataset to the agent who created it\n",
    "  for ml_algorithm, ml_algorithm_version in ml_algorithms:  # For each machine learning algorithm...\n",
    "    algo = d1.activity('sk:' + ml_algorithm_version)  # register the algorithm as a (processing) activity\n",
    "    processed_dataset = d1.entity('unibo:' + ml_algorithm_version + \"_\" + dataset_version, {'sk:cv-score': '...'})  # create an activity represented the processed dataset\n",
    "    d1.used(algo, original_dataset)  # the activity used the dataset as input\n",
    "    d1.wasGeneratedBy(processed_dataset, algo)  # the processed dataset has been created by the algorithm\n",
    "    d1.wasDerivedFrom(processed_dataset, original_dataset)  # the processed dataset has been derived from the original one\n",
    "dot = prov_to_dot(d1)  # visualize the graph\n",
    "dot.write_png('prov.png')\n",
    "Image('prov.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lJyR1lr_uPE"
   },
   "source": [
    "# Can you build and track a better model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzzwzxQP_02f"
   },
   "outputs": [],
   "source": [
    "# Try your sk-learn model here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
